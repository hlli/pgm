% function [nll, grad] = InstanceNegLogLikelihood(X, y, theta, modelParams)
% returns the negative log-likelihood and its gradient, given a CRF with parameters theta,
% on data (X, y). 
%
% Inputs:
% X            Data.                           (numCharacters x numImageFeatures matrix)
%              X(:,1) is all ones, i.e., it encodes the intercept/bias term.
% y            Data labels.                    (numCharacters x 1 vector)
% theta        CRF weights/parameters.         (numParams x 1 vector)
%              These are shared among the various singleton / pairwise features.
% modelParams  Struct with three fields:
%   .numHiddenStates     in our case, set to 26 (26 possible characters)
%   .numObservedStates   in our case, set to 2  (each pixel is either on or off)
%   .lambda              the regularization parameter lambda
%
% Outputs:
% nll          Negative log-likelihood of the data.    (scalar)
% grad         Gradient of nll with respect to theta   (numParams x 1 vector)
%
% Copyright (C) Daphne Koller, Stanford Univerity, 2012

function [nll, grad] = InstanceNegLogLikelihood(X, y, theta, modelParams)

    % featureSet is a struct with two fields:
    %    .numParams - the number of parameters in the CRF (this is not numImageFeatures
    %                 nor numFeatures, because of parameter sharing)
    %    .features  - an array comprising the features in the CRF.
    %
    % Each feature is a binary indicator variable, represented by a struct 
    % with three fields:
    %    .var          - a vector containing the variables in the scope of this feature
    %    .assignment   - the assignment that this indicator variable corresponds to
    %    .paramIdx     - the index in theta that this feature corresponds to
    %
    % For example, if we have:
    %   
    %   feature = struct('var', [2 3], 'assignment', [5 6], 'paramIdx', 8);
    %
    % then feature is an indicator function over X_2 and X_3, which takes on a value of 1
    % if X_2 = 5 and X_3 = 6 (which would be 'e' and 'f'), and 0 otherwise. 
    % Its contribution to the log-likelihood would be theta(8) if it's 1, and 0 otherwise.
    %
    % If you're interested in the implementation details of CRFs, 
    % feel free to read through GenerateAllFeatures.m and the functions it calls!
    % For the purposes of this assignment, though, you don't
    % have to understand how this code works. (It's complicated.)
    
    featureSet = GenerateAllFeatures(X, modelParams);
    features = featureSet.features;
    
    [F, vars] = ConvertFeaturesToFactorList(features, modelParams.numHiddenStates, theta);
    T = CreateCliqueTree(F);
    [T, logZ] = CliqueTreeCalibrate(T, 0);
    
    n = length(theta);
    m = length(features);
    dataFeatureCounts = zeros(1, n);
    weightedFeatureCounts = zeros(1, n);
    mapFeaturesToTheta = cell(1, n);
    
    for i = 1:n
        for j = 1:m
            if (features(j).paramIdx == i)
                mapFeaturesToTheta{i} = [mapFeaturesToTheta{i}, j];
                if all(features(j).assignment == y(features(j).var))
                    dataFeatureCounts(i) = dataFeatureCounts(i) + 1;
                end
            end
        end
    end
    
    weightedFeatureCounts = theta .* dataFeatureCounts;
    
    D = ComputeDistribution(vars, T.cliqueList);
    modelExpectedFeatureCounts = zeros(1, n);
    
    for i = 1:n
        if (~isempty(mapFeaturesToTheta{i}))
            featureIdx = mapFeaturesToTheta{i};
            for k = 1:length(featureIdx)
                idx = featureIdx(k);
                for l = 1:length(D)
                    if (D(l).var == features(idx).var)
                        factorIndex = AssignmentToIndex(features(idx).assignment, D(l).card);
                        modelExpectedFeatureCounts(i) = modelExpectedFeatureCounts(i) + D(l).val(factorIndex);
                    end
                end
            end
        end
    end
        
        
    % Use the featureSet to calculate nll and grad.
    % This is the main part of the assignment, and it is very tricky - be careful!
    % You might want to code up your own numerical gradient checker to make sure
    % your answers are correct.
    %
    % Hint: you can use CliqueTreeCalibrate to calculate logZ effectively. 
    %       We have halfway-modified CliqueTreeCalibrate; complete our implementation 
    %       if you want to use it to compute logZ.
    nll = 0;
    grad = zeros(size(theta));
    %%%
    % Your code here:
    
    weightedCounts = 0;
    penalty = 0;
    for i = 1:n
        weightedCounts = weightedCounts + weightedFeatureCounts(i);
        penalty = penalty + theta(i)^2;
    end
    
    nll = logZ - weightedCounts + (modelParams.lambda / 2) * penalty;
    
    grad = modelExpected
    
    
end


function [F, vars] = ConvertFeaturesToFactorList(features, numHiddenStates, theta)
    n = length(features);
    vars = {};
    vars{1} = features(1).var;
    len = length(vars);
    for i = 2:n
        Exist = 0;
        for j = 1:len
            if (features(i).var == vars{j})
                Exist = 1;
                break
            end
        end
        if (Exist == 0)
            len = len + 1;
            vars{len} = features(i).var;
        end
    end
    
    N = length(vars);
    F = repmat(struct('var', [], 'card', [], 'val', []), 1, N);
    
    for i = 1:N
        F(i).var = vars{i};
        F(i).card = numHiddenStates * ones(size(vars{i}));
        F(i).val = zeros(1, prod(F(i).card));
        for j = 1:n
            if all(features(j).var == F(i).var)
                idx = AssignmentToIndex(features(j).assignment, F(i).card);
                F(i).val(idx) = F(i).val(idx) + theta(features(j).paramIdx);
            end
        end
        F(i).val = exp(F(i).val);
    end
    
end



function D = ComputeDistribution(vars, cliqueList)
    N = length(vars);
    D = repmat(struct('var', [], 'card', [], 'val', []), 1, N);
    n = length(cliqueList);
    
    %for k = 1:n 
        %cliqueList(k).val = cliqueList(k).val / sum(cliqueList(k).val);
    %end
    
    for i = 1:N
        for j = 1:n
            if all(ismember(vars{i}, cliqueList(j).var))
                diff = setdiff(cliqueList(j).var, vars{i});
                D(i) = FactorMarginalization(cliqueList(j), diff);
                D(i).val = D(i).val / sum(D(i).val);
                break
            end
        end
    end
    
end
            






